外部ツール使用に関しての進め方を記載します。

1.LM studioのインストール
これは様々な公開モデルをローカル環境で動かせるアプリです。
https://lmstudio.ai/
このサイトからwindows版をダウンロードします。
いくつか初期設定がありますが、choose your levelではDeveloperにしておきます。

外部ツール使用のためのモデルとして、
llama-3-8b-instruct-32k-v0.1-GGUF
を採用しました。
AIによるとinstructなどがついているモデルだと外部ツール使用を促しやすいとのことでした。

imgフォルダ内にモデルダウンロードの画像、開発者画面の画像、サーバ起動の画像があるので参照してください。
モデルダウンロードは検索窓で llama-3-8b-instruct-32k-v0.1-GGUF と打てば出てきます。
5GB近くあるので容量が必要です。開発者画面へはウィンドウ左の開発者から遷移できます。
開発者画面の左上の Status:Runnning をオンにするとサーバが起動します。
また、サーバを起動すると画面右中央にAPI Usageで、API identifierが表示されます。

test.pyではAPI identifierから、
"model": "llama-3-8b-instruct-32k-v0.1",
を指定し、ローカルホストとして、
API_URL = "http://127.0.0.1:1234/v1/completions"
を使用しています。

LM studioでサーバを起動したら、
test.pyを実行すると、モデルとの対話ができ、簡単な計算3+4などを入力すると答えが返ってきます。